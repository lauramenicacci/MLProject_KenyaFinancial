{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":1969,"status":"ok","timestamp":1649697217937,"user":{"displayName":"Laura Menicacci","userId":"16575933074778738670"},"user_tz":-120},"id":"m2Fo_WbDQ9zF","outputId":"b2ace293-849f-41de-dddf-de5ec9b6ddc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/GRAD-C24_Machine_Learning/MLProject_KenyaFinancial\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/GRAD-C24_Machine_Learning/MLProject_KenyaFinancial'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}],"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# Common imports\n","import pandas as pd \n","import numpy as np\n","import os\n","\n","#import data \n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/GRAD-C24_Machine_Learning/MLProject_KenyaFinancial\n","DATA_PATH = \"/content/drive/MyDrive/GRAD-C24_Machine_Learning/MLProject_KenyaFinancial/raw_data/\"\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5114,"status":"ok","timestamp":1649697225827,"user":{"displayName":"Laura Menicacci","userId":"16575933074778738670"},"user_tz":-120},"id":"xIEnjUDvSFkI","outputId":"6273ef29-062e-453c-f2f2-71baf682d499"},"outputs":[{"output_type":"stream","name":"stdout","text":["['con_prod_hh_over_trx_yr_mo_usd.csv', 'con_pur_hh_by_trx_yr_mo_usd.csv', 'demo_hh.csv', 'demo_ind.csv', 'diaries_goingson_hh.csv', 'diaries_wellbeing_ind.csv', 'edu_hh.csv', 'edu_ind.csv', 'housing_conditions_ownership.csv', 'inc_hh_by_trx_yr_mo_usd_incRR.csv', 'inc_ind_by_trx_yr_mo_usd_incRR.csv', 'pov_hh_by_trx_yr_mo_usd_incRR.csv', 'rem_hh_by_trx_yr_mo_usd.csv', 'pov_hh_by_trx_yr_mo_usd_excRR.csv', 'diaries_transactions_all.csv']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}],"source":["from os import listdir\n","FILE_LIST = listdir(DATA_PATH)\n","print(FILE_LIST)\n","\n","#test\n","#FILE_LIST[0].split(\".csv\")[0]\n","\n","df_dic={} #create a dictionary with names of datasets as keys and datasets as values\n","for i, e in enumerate(FILE_LIST):\n","  df_dic[str(FILE_LIST[i].split(\".csv\")[0])]= pd.read_csv(\"raw_data/\" + e)\n","\n","#test\n","#df_dic['diaries_transactions_all'].columns"]},{"cell_type":"code","source":["#start merging\n","#first we merge datasets with same keys together, then we merge the bigger (already merged) datasets together\n","from functools import reduce\n","\n","dfs = [df_dic['diaries_transactions_all'], df_dic['demo_hh'] , df_dic['con_prod_hh_over_trx_yr_mo_usd'], df_dic['edu_hh']]\n","\n","result_hhids = reduce(lambda df_left, df_right: pd.merge(df_left, df_right, \n","                                              right_on = 'hh_ids', left_on = 'hh_ids', \n","                                              how='outer'), \n","                  dfs)"],"metadata":{"id":"xe_y2_mS4f5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result_hhids)"],"metadata":{"id":"iFi63dvu4hnG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfs = [df_dic['con_pur_hh_by_trx_yr_mo_usd'], df_dic['inc_hh_by_trx_yr_mo_usd_incRR'], df_dic['pov_hh_by_trx_yr_mo_usd_excRR'], df_dic['pov_hh_by_trx_yr_mo_usd_incRR'], df_dic['rem_hh_by_trx_yr_mo_usd']]\n","\n","result_hhids_trxyrmo = reduce(lambda df_left, df_right: pd.merge(df_left, df_right, \n","                                              right_on = ['hh_ids', 'trx_yr_mo'], left_on =['hh_ids', 'trx_yr_mo'], \n","                                              how= 'outer'),\n","                  dfs)"],"metadata":{"id":"cIv-WnQpIJpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result_hhids_trxyrmo)"],"metadata":{"id":"EqI9E9wI-sXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfs = [df_dic['diaries_wellbeing_ind'], df_dic['diaries_goingson_hh']]\n","\n","result_hhids_intdate = reduce(lambda df_left, df_right: pd.merge(df_left, df_right, \n","                                              right_on = ['hh_ids', 'int_date'], left_on =['hh_ids', 'int_date'], \n","                                              how= 'outer'),\n","                  dfs)\n"],"metadata":{"id":"pa1uFBBXOUOB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result_hhids_intdate)"],"metadata":{"id":"8yQ7uwzycfjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfs = [df_dic['demo_ind'], df_dic['edu_ind']]\n","\n","result_hhids_inds = reduce(lambda df_left, df_right: pd.merge(df_left, df_right, \n","                                              right_on = ['hh_ids', 'm_ids'], left_on =['hh_ids', 'm_ids'], \n","                                              how= 'outer'),\n","                  dfs)\n","\n","#convert to string for merging purposes\n","result_hhids_inds['m_ids'] = result_hhids_inds['m_ids'].astype(str)"],"metadata":{"id":"fafdf5Hccliv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result_hhids_inds)"],"metadata":{"id":"iwx8atNjc-Sc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#final part of merging\n","official_1 = pd.merge(result_hhids, result_hhids_trxyrmo, right_on = ['hh_ids', 'trx_yr_mo'], left_on =['hh_ids', 'trx_yr_mo'], \n","                                              how= 'outer')\n","\n"],"metadata":{"id":"zHWnBbK1o8gY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["official_2 = pd.merge(official_1, result_hhids_intdate, right_on = ['hh_ids', 'int_date'], left_on =['hh_ids', 'int_date'], \n","                                              how= 'outer')"],"metadata":{"id":"ntS-ajH-qGNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["official= pd.merge(official_2, result_hhids_inds, right_on = ['hh_ids', 'm_ids'], left_on =['hh_ids', 'm_ids_owner'], \n","                                              how= 'outer')\n"],"metadata":{"id":"26o23wjkqIz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["official.head()"],"metadata":{"id":"66bLK5hqiKzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["official[official['hh_ids'] == 'KELDK01']"],"metadata":{"id":"Glkeg7tGi1A8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["final comment on merging --> DID NOT MANAGE TO MERGE inc_ind_by_trx_yr_mo_usd_incRR and housing_conditions --> remember to include limitations of this choice in our report!"],"metadata":{"id":"bd6h5i4_ovci"}},{"cell_type":"code","source":["#replacing NAs consistently along the whole dataset\n","official.replace('.', np.NaN)\n","official.replace(' ', np.NaN)"],"metadata":{"id":"S7AuJkrTWBJn","colab":{"base_uri":"https://localhost:8080/","height":834},"executionInfo":{"status":"ok","timestamp":1649697250300,"user_tz":-120,"elapsed":4452,"user":{"displayName":"Laura Menicacci","userId":"16575933074778738670"}},"outputId":"832837f1-3212-420f-d96b-620e44b10367"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         hh_ids  unique_hhs first_trx_date_hh last_trx_date_hh  \\\n","0       KELDL02         NaN         03sep2012        02oct2013   \n","1       KELDL02         NaN         03sep2012        02oct2013   \n","2       KELDL02         NaN         03sep2012        02oct2013   \n","3       KELDL02         NaN         03sep2012        02oct2013   \n","4       KELDL02         NaN         03sep2012        02oct2013   \n","...         ...         ...               ...              ...   \n","735844  KVIHK39         NaN               NaN              NaN   \n","735845  KVIHK39         NaN               NaN              NaN   \n","735846  KVIHK40         NaN               NaN              NaN   \n","735847  KVIHK40         NaN               NaN              NaN   \n","735848  KZIMA08         NaN               NaN              NaN   \n","\n","        tot_hh_daysofobs  tot_hh_monthsofobs interview_designation   int_date  \\\n","0                  394.0                13.0  04=Diaries Interview  02oct2013   \n","1                  394.0                13.0  04=Diaries Interview  02oct2013   \n","2                  394.0                13.0  04=Diaries Interview  02oct2013   \n","3                  394.0                13.0  04=Diaries Interview  02oct2013   \n","4                  394.0                13.0  04=Diaries Interview  02oct2013   \n","...                  ...                 ...                   ...        ...   \n","735844               NaN                 NaN                   NaN        NaN   \n","735845               NaN                 NaN                   NaN        NaN   \n","735846               NaN                 NaN                   NaN        NaN   \n","735847               NaN                 NaN                   NaN        NaN   \n","735848               NaN                 NaN                   NaN        NaN   \n","\n","        int_month  int_year  ... dem_i_age_workage    dem_i_relhead  \\\n","0            10.0    2013.0  ...               1.0   Household head   \n","1            10.0    2013.0  ...               1.0   Household head   \n","2            10.0    2013.0  ...               1.0   Household head   \n","3            10.0    2013.0  ...               1.0   Household head   \n","4            10.0    2013.0  ...               1.0   Household head   \n","...           ...       ...  ...               ...              ...   \n","735844        NaN       NaN  ...               0.0  Son or daughter   \n","735845        NaN       NaN  ...               0.0  Son or daughter   \n","735846        NaN       NaN  ...               0.0  Son or daughter   \n","735847        NaN       NaN  ...               0.0  Son or daughter   \n","735848        NaN       NaN  ...               0.0  Son or daughter   \n","\n","       dem_i_hhead                 dem_i_marstat dem_i_tribe  \\\n","0              1.0       Married/living together         Luo   \n","1              1.0       Married/living together         Luo   \n","2              1.0       Married/living together         Luo   \n","3              1.0       Married/living together         Luo   \n","4              1.0       Married/living together         Luo   \n","...            ...                           ...         ...   \n","735844         0.0  Never married/lived together       Luhya   \n","735845         0.0  Never married/lived together       Luhya   \n","735846         0.0  Never married/lived together       Luhya   \n","735847         0.0  Never married/lived together       Luhya   \n","735848         0.0  Never married/lived together        Embu   \n","\n","        dem_i_children_tot dem_i_children_inhh edu_i_enrolled  \\\n","0                      2.0                 0.0            0.0   \n","1                      2.0                 0.0            0.0   \n","2                      2.0                 0.0            0.0   \n","3                      2.0                 0.0            0.0   \n","4                      2.0                 0.0            0.0   \n","...                    ...                 ...            ...   \n","735844                 NaN                 NaN            1.0   \n","735845                 NaN                 NaN            1.0   \n","735846                 NaN                 NaN            1.0   \n","735847                 NaN                 NaN            1.0   \n","735848                 NaN                 NaN            0.0   \n","\n","                        edu_i_attain edu_i_attain_yrs  \n","0       Secondary (some or complete)             12.0  \n","1       Secondary (some or complete)             12.0  \n","2       Secondary (some or complete)             12.0  \n","3       Secondary (some or complete)             12.0  \n","4       Secondary (some or complete)             12.0  \n","...                              ...              ...  \n","735844         Nursery/ Kindergarten              0.0  \n","735845                           NaN              NaN  \n","735846         Nursery/ Kindergarten              0.0  \n","735847                  No education              0.0  \n","735848                  No education              0.0  \n","\n","[735849 rows x 203 columns]"],"text/html":["\n","  <div id=\"df-859d1126-b80a-459f-af21-905fbfbd835f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hh_ids</th>\n","      <th>unique_hhs</th>\n","      <th>first_trx_date_hh</th>\n","      <th>last_trx_date_hh</th>\n","      <th>tot_hh_daysofobs</th>\n","      <th>tot_hh_monthsofobs</th>\n","      <th>interview_designation</th>\n","      <th>int_date</th>\n","      <th>int_month</th>\n","      <th>int_year</th>\n","      <th>...</th>\n","      <th>dem_i_age_workage</th>\n","      <th>dem_i_relhead</th>\n","      <th>dem_i_hhead</th>\n","      <th>dem_i_marstat</th>\n","      <th>dem_i_tribe</th>\n","      <th>dem_i_children_tot</th>\n","      <th>dem_i_children_inhh</th>\n","      <th>edu_i_enrolled</th>\n","      <th>edu_i_attain</th>\n","      <th>edu_i_attain_yrs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>KELDL02</td>\n","      <td>NaN</td>\n","      <td>03sep2012</td>\n","      <td>02oct2013</td>\n","      <td>394.0</td>\n","      <td>13.0</td>\n","      <td>04=Diaries Interview</td>\n","      <td>02oct2013</td>\n","      <td>10.0</td>\n","      <td>2013.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>Household head</td>\n","      <td>1.0</td>\n","      <td>Married/living together</td>\n","      <td>Luo</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Secondary (some or complete)</td>\n","      <td>12.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>KELDL02</td>\n","      <td>NaN</td>\n","      <td>03sep2012</td>\n","      <td>02oct2013</td>\n","      <td>394.0</td>\n","      <td>13.0</td>\n","      <td>04=Diaries Interview</td>\n","      <td>02oct2013</td>\n","      <td>10.0</td>\n","      <td>2013.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>Household head</td>\n","      <td>1.0</td>\n","      <td>Married/living together</td>\n","      <td>Luo</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Secondary (some or complete)</td>\n","      <td>12.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>KELDL02</td>\n","      <td>NaN</td>\n","      <td>03sep2012</td>\n","      <td>02oct2013</td>\n","      <td>394.0</td>\n","      <td>13.0</td>\n","      <td>04=Diaries Interview</td>\n","      <td>02oct2013</td>\n","      <td>10.0</td>\n","      <td>2013.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>Household head</td>\n","      <td>1.0</td>\n","      <td>Married/living together</td>\n","      <td>Luo</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Secondary (some or complete)</td>\n","      <td>12.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>KELDL02</td>\n","      <td>NaN</td>\n","      <td>03sep2012</td>\n","      <td>02oct2013</td>\n","      <td>394.0</td>\n","      <td>13.0</td>\n","      <td>04=Diaries Interview</td>\n","      <td>02oct2013</td>\n","      <td>10.0</td>\n","      <td>2013.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>Household head</td>\n","      <td>1.0</td>\n","      <td>Married/living together</td>\n","      <td>Luo</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Secondary (some or complete)</td>\n","      <td>12.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>KELDL02</td>\n","      <td>NaN</td>\n","      <td>03sep2012</td>\n","      <td>02oct2013</td>\n","      <td>394.0</td>\n","      <td>13.0</td>\n","      <td>04=Diaries Interview</td>\n","      <td>02oct2013</td>\n","      <td>10.0</td>\n","      <td>2013.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>Household head</td>\n","      <td>1.0</td>\n","      <td>Married/living together</td>\n","      <td>Luo</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Secondary (some or complete)</td>\n","      <td>12.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>735844</th>\n","      <td>KVIHK39</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>Son or daughter</td>\n","      <td>0.0</td>\n","      <td>Never married/lived together</td>\n","      <td>Luhya</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>Nursery/ Kindergarten</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>735845</th>\n","      <td>KVIHK39</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>Son or daughter</td>\n","      <td>0.0</td>\n","      <td>Never married/lived together</td>\n","      <td>Luhya</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>735846</th>\n","      <td>KVIHK40</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>Son or daughter</td>\n","      <td>0.0</td>\n","      <td>Never married/lived together</td>\n","      <td>Luhya</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>Nursery/ Kindergarten</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>735847</th>\n","      <td>KVIHK40</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>Son or daughter</td>\n","      <td>0.0</td>\n","      <td>Never married/lived together</td>\n","      <td>Luhya</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>No education</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>735848</th>\n","      <td>KZIMA08</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>Son or daughter</td>\n","      <td>0.0</td>\n","      <td>Never married/lived together</td>\n","      <td>Embu</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>No education</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>735849 rows × 203 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-859d1126-b80a-459f-af21-905fbfbd835f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-859d1126-b80a-459f-af21-905fbfbd835f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-859d1126-b80a-459f-af21-905fbfbd835f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["#checking for empty columns \n","empty_cols = [col for col in official.columns if official[col].isnull().all()]\n","\n","print(empty_cols) #there are no empty columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"skstbT1EtTw6","executionInfo":{"status":"ok","timestamp":1649695744771,"user_tz":-120,"elapsed":2096,"user":{"displayName":"Laura Menicacci","userId":"16575933074778738670"}},"outputId":"56c41a5a-e3a4-433e-b788-88179b82ecf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"code","source":["#dropping unimportant cols \n","\n","official = official.drop(columns = ['first_trx_date_hh', 'last_trx_date_hh', 'tot_hh_daysofobs', 'tot_hh_monthsofobs', 'interview_designation', 'int_date', 'int_month', 'int_year', 'int_yr_mo', 'first_int_date', 'unique_accnts', 'unique_hm_owner', 'tot_acc_daysofobs', 'tot_acc_monthsofobs', 'trx_stdtime_days_hh', 'trx_stdtime_mnths_hh', 'trx_stdtime_days_acc', 'trx_stdtime_mnths_acc', 'trx_family_desc', 'trx_class_desc', 'trx_type_desc', 'trx_mode_code', 'trx_place_incommunity', 'trx_distance_km', 'trx_value_kes', 'trx_inkind_units', 'trx_inkind_value_kes'])\n"],"metadata":{"id":"eW1Il3OJLfZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dropping cols with more than 80% of NaNs\n","\n","na_perc = official[official.columns[official.isnull().any()]].isnull().sum() * 100 / official.shape[0]\n","#print(na_perc)\n","\n","nas_list = []\n","for i in na_perc.keys():\n","  if na_perc[i] > 80.0:\n","    nas_list.append(i)\n","\n","#print(nas_list)"],"metadata":{"id":"RDAu0wq5RMsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DROP MOSTLY EMPTY COLUMNS\n","official = official.drop(columns = nas_list)\n","\n","#official.head()"],"metadata":{"id":"c1J__UnLi52p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perc = 80.0\n","min_count =  int(((100-perc)/100)*official.shape[1] + 1)\n","official_rowstry = official.dropna( axis=0, \n","                    thresh=min_count)"],"metadata":{"id":"RHkXf0DZhdOJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(official_rowstry)"],"metadata":{"id":"SLA4Ku-NnQIz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Merged dataframe is ready to be split into train and test!"],"metadata":{"id":"X5MJPlVHvIPC"}},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kgxE7ZoLEXTg","executionInfo":{"status":"ok","timestamp":1649697373230,"user_tz":-120,"elapsed":300,"user":{"displayName":"Laura Menicacci","userId":"16575933074778738670"}},"outputId":"9a8b0868-ef72-4626-c21c-4e9294b8c1db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/GRAD-C24_Machine_Learning/MLProject_KenyaFinancial'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["os.makedirs('clean_data', exist_ok=True)  \n","compression_opts = dict(method='zip',\n","                        archive_name='official.csv')\n","official.to_csv('clean_data/official.zip', index=False,\n","          compression=compression_opts) "],"metadata":{"id":"PDz9agcREQow"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"KF_cleaning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}